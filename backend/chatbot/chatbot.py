import os
import time
from dotenv import load_dotenv

import streamlit as st
from langchain_upstage import UpstageEmbeddings, ChatUpstage
from langchain_chroma import Chroma
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

load_dotenv()

# â–¶ï¸ API í‚¤ í™•ì¸
api_key = os.getenv("UPSTAGE_API_KEY")
if not api_key:
    st.error("â—ï¸Upstage API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    st.stop()

# â–¶ï¸ ë²¡í„°ìŠ¤í† ì–´ ë¡œë”©
embedding = UpstageEmbeddings(
    model="solar-embedding-1-large",
    upstage_api_key=api_key
)

vectorstore = Chroma(
    persist_directory="chroma_db",
    embedding_function=embedding
)
retriever = vectorstore.as_retriever(k=2)

# â–¶ï¸ ì±—ë´‡ ì´ˆê¸°í™”
chat = ChatUpstage(
    model="solar-pro",
    upstage_api_key=api_key
)

# â–¶ï¸ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
contextualize_q_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "ì´ì „ ëŒ€í™” ë‚´ìš©ê³¼ ì‚¬ìš©ì ì§ˆë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ, ë¬¸ë§¥ ì—†ì´ë„ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì§ˆë¬¸ì„ ë‹¤ì‹œ í‘œí˜„í•˜ì„¸ìš”."),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ]
)
history_aware_retriever = create_history_aware_retriever(chat, retriever, contextualize_q_prompt)

qa_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", 
         """ì§ˆë¬¸ì— ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”. ë‹µì„ ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  í•˜ì„¸ìš”. ë‹µë³€ì€ ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”. ë²•ë¥ ê³¼ ê´€ë ¨ëœ ì§ˆë¬¸ì—ë§Œ ë‹µë³€í•˜ì„¸ìš”. ê´€ë ¨ë˜ì§€ ì•Šì„ê²½ìš° ë²•ë¥ ê³¼ ê´€ë ¨ëœ ì§ˆë¬¸ì„ í•  ìˆ˜ ìˆë„ë¡ ìœ ë„í•˜ëŠ” ë§ì„ í•´ì£¼ì„¸ìš”.

ğŸ“ë‹µë³€ ë‚´ìš©:  
ğŸ“ì¶œì²˜ ë¬¸ì„œ: {context}"""),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ]
)

question_answer_chain = create_stuff_documents_chain(chat, qa_prompt)
rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)

# â–¶ï¸ Streamlit UI
st.title("ğŸ“š LawRo ë²•ë¥  ìƒë‹´ Chatbot")

# â–¶ï¸ ëŒ€í™” ìƒíƒœ ê´€ë¦¬
if "messages" not in st.session_state:
    st.session_state.messages = []

# â–¶ï¸ ì´ì „ ë©”ì‹œì§€ ì¶œë ¥
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# â–¶ï¸ ì…ë ¥ ì²˜ë¦¬
MAX_MESSAGES = 6

if prompt := st.chat_input("ë²•ë¥  ê´€ë ¨ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”."):
    st.chat_message("user").markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    # â–¶ï¸ ìµœê·¼ Nê°œë§Œ ìœ ì§€
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""

        result = rag_chain.invoke({
            "input": prompt,
            "chat_history": st.session_state.messages
        })

        # â–¶ï¸ ì¦ê±°ìë£Œ ë‚´ìš© ì¶œë ¥
        docs = result.get("context", [])
        with st.expander("ğŸ“‚ ê´€ë ¨ ë¬¸ì„œ"):
            if isinstance(docs, list):
                for i, doc in enumerate(docs, 1):
                    content = getattr(doc, "page_content", "ì¶œì²˜ ì—†ìŒ")
                    st.markdown(f"**[{i}]** {content.strip()[:500]}...")  # ìµœëŒ€ 500ì

        # â–¶ï¸ ë‹µë³€ ì• ë‹ˆë©”ì´ì…˜ ì¶œë ¥
        for chunk in result["answer"].split(" "):
            full_response += chunk + " "
            message_placeholder.markdown(full_response + "â–Œ")
            time.sleep(0.02)
        message_placeholder.markdown(full_response)

    st.session_state.messages.append({"role": "assistant", "content": full_response})
