import streamlit as st
import requests
import cv2
import numpy as np
import json
import os
from PIL import Image
from dotenv import load_dotenv
from langchain_upstage import UpstageEmbeddings, ChatUpstage
from langchain_community.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ì„¤ì •
API_KEY = os.getenv("UPSTAGE_API_KEY")
OCR_URL = "https://ap-northeast-2.apistage.ai/v1/document-ai/ocr"
OCR_HEADERS = {"Authorization": f"Bearer {API_KEY}"}


# âœ… OCR ìš”ì²­
def run_ocr(uploaded_file):
    files = {"image": (uploaded_file.name, uploaded_file.getvalue())}
    response = requests.post(OCR_URL, headers=OCR_HEADERS, files=files)
    return response


# âœ… ë°•ìŠ¤ ì‹œê°í™”
def highlight_boxes(image, ocr_data):
    for page in ocr_data.get("pages", []):
        for word in page.get("words", []):
            vertices = word["boundingBox"]["vertices"]
            pt1 = (vertices[0]["x"], vertices[0]["y"])
            pt2 = (vertices[2]["x"], vertices[2]["y"])
            cv2.rectangle(image, pt1, pt2, (0, 0, 255), 2)
    return image


# âœ… ë²¡í„°DB + LLM RAG êµ¬ì„±
@st.cache_resource
def setup_rag():
    embedding = UpstageEmbeddings(model="solar-embedding-1-large", upstage_api_key=API_KEY)
    vectorstore = Chroma(persist_directory="chroma_db", embedding_function=embedding)

    llm = ChatUpstage(model="solar-pro", upstage_api_key=API_KEY, temperature=0.2)

    prompt = PromptTemplate(
        template="""
ë‹¤ìŒì€ OCRë¡œ ì¶”ì¶œëœ ê·¼ë¡œê³„ì•½ì„œ ë‚´ìš©ì…ë‹ˆë‹¤. ì•„ë˜ ë²•ë¥  ê·¼ê±°ì™€ ë¹„êµí•˜ì—¬
ìœ„ë°˜ ì—¬ë¶€ë¥¼ íŒë‹¨í•´ì£¼ì„¸ìš”. ìœ„ë°˜ì´ë¼ë©´ ì–´ë–¤ ì¡°í•­ì„ ìœ„ë°˜í–ˆëŠ”ì§€ ì„¤ëª…í•´ì£¼ì„¸ìš”.

[ê³„ì•½ì„œ ë‚´ìš©]
{question}

[ë²•ë¥  ê·¼ê±°]
{context}

ë‹¹ì‹ ì˜ íŒë‹¨:
""",
        input_variables=["question", "context"]
    )

    qa = RetrievalQA.from_chain_type(
        llm=llm,
        retriever=vectorstore.as_retriever(search_kwargs={"k": 4}),
        chain_type="stuff",
        chain_type_kwargs={"prompt": prompt},
        return_source_documents=True
    )
    return qa


# âœ… Streamlit ì•± ì‹œì‘
st.set_page_config(page_title="ê·¼ë¡œê³„ì•½ì„œ ìœ„ë°˜ ë¶„ì„ (RAG)", layout="wide")
st.title("ğŸ“„ ê·¼ë¡œê³„ì•½ì„œ ë²•ë¥  ìœ„ë°˜ ë¶„ì„ (Upstage OCR + RAG)")

uploaded_file = st.file_uploader("ğŸ“¤ ê·¼ë¡œê³„ì•½ì„œ ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš” (.png, .jpg, .jpeg)", type=["png", "jpg", "jpeg"])

if uploaded_file:
    image = Image.open(uploaded_file).convert("RGB")
    np_image = np.array(image)
    st.image(image, caption="ì›ë³¸ ì´ë¯¸ì§€", use_column_width=True)

    with st.spinner("ğŸ§  OCR ìˆ˜í–‰ ì¤‘..."):
        response = run_ocr(uploaded_file)

        if response.status_code == 200:
            ocr_result = response.json()
            full_text = " ".join([p.get("text", "") for p in ocr_result.get("pages", [])])

            annotated_image = highlight_boxes(np_image.copy(), ocr_result)
            st.image(annotated_image, caption="ğŸ”´ í…ìŠ¤íŠ¸ ê°•ì¡° ì´ë¯¸ì§€", use_column_width=True)

            # âœ… ë²•ë¥  ê·¼ê±° ê¸°ë°˜ ë¶„ì„ (RAG)
            with st.spinner("âš–ï¸ ë²•ë¥  ê·¼ê±° ê¸°ë°˜ ìœ„ë°˜ ë¶„ì„ ì¤‘..."):
                rag_chain = setup_rag()
                result = rag_chain({"query": full_text})

                st.subheader("ğŸ§  íŒë‹¨ ê²°ê³¼")
                st.markdown(result["result"])

                st.subheader("ğŸ“š ì°¸ì¡°í•œ ë²•ë¥  ì¡°í•­ë“¤")
                for i, doc in enumerate(result["source_documents"]):
                    st.markdown(f"**{i+1}. {doc.metadata.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ')}**")
                    st.code(doc.page_content.strip(), language="markdown")
        else:
            st.error(f"OCR ì‹¤íŒ¨: {response.status_code}")
            st.text(response.text)
