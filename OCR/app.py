import streamlit as st
import requests
import json
import re
from rapidfuzz import fuzz
from rag_utils import get_rag_result
from llm_utils import get_llm_judgment
from pdf_export import export_pdf
from dotenv import load_dotenv
import os
load_dotenv()

# í•´ë‹¹ .env íŒŒì¼ì— UPSTAGE_API_KEY í•´ë‹¹ 
API_KEY = os.getenv("UPSTAGE_API_KEY")
OCR_URL = "https://ap-northeast-2.apistage.ai/v1/document-ai/ocr"
HEADERS = {"Authorization": f"Bearer {API_KEY}"}

# ---- ê°€ì¥ ê¸°ë³¸ì ì¸ ì •ê·œì‹ ê¸°ë³¸ í˜•ì‹ ë°©ì‹ ----
def preprocess(text):
    lines = text.splitlines()
    out = []
    for line in lines:
        line = re.sub(r"[â– â—â˜‘ï¸âœ”ï¸]", "[X]", line)
        line = re.sub(r"[â–¡â¬œï¸â˜]", "[ ]", line)
        line = line.strip()
        if line and not re.match(r"^[-=]{3,}$", line):
            out.append(line)
    return "\n".join(out)

def is_similar(word, keyword, threshold=85):
    return fuzz.ratio(word.lower(), keyword.lower()) >= threshold or keyword.lower() in word.lower()

required_fields = {
    "ì‚¬ìš©ì ì •ë³´": ["ì„±ëª…", "ì—…ì²´ëª…", "ì†Œì¬ì§€", "ì „í™”ë²ˆí˜¸", "ì‚¬ì—…ìë“±ë¡ë²ˆí˜¸", "ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸"],
    "ê·¼ë¡œì ì •ë³´": ["ê·¼ë¡œì", "ì„±ëª…", "ìƒë…„ì›”ì¼", "ë³¸êµ­ì£¼ì†Œ"],
    "ê·¼ë¡œê³„ì•½ê¸°ê°„": ["ê·¼ë¡œê³„ì•½ê¸°ê°„", "ìˆ˜ìŠµê¸°ê°„", "ì‹ ê·œ", "ì¬ì…êµ­"],
    "ê·¼ë¡œì¥ì†Œ": ["ê·¼ë¡œì¥ì†Œ", "ì¥ì†Œ"],
    "ì—…ë¬´ë‚´ìš©": ["ì—…ì¢…", "ì‚¬ì—…ë‚´ìš©", "ì§ë¬´ë‚´ìš©"],
    "ê·¼ë¡œì‹œê°„": ["ê·¼ë¡œì‹œê°„", "ì‹œ", "ë¶„", "êµëŒ€ì œ"],
    "íœ´ê²Œì‹œê°„": ["íœ´ê²Œì‹œê°„", "ë¶„"],
    "íœ´ì¼": ["íœ´ì¼", "ì¼ìš”ì¼", "í† ìš”ì¼", "ê³µíœ´ì¼", "ìœ ê¸‰", "ë¬´ê¸‰"]
}

law_violation_rules = {
    "ìˆ˜ìŠµê¸°ê°„ 6ê°œì›” ì´ìƒ": lambda t: re.search(r"ìˆ˜ìŠµ.*(6ê°œì›”|180ì¼|ì´ìƒ)", t),
    "ì£¼ 52ì‹œê°„ ì´ˆê³¼ ê·¼ë¬´": lambda t: re.search(r"(52ì‹œê°„|í•˜ë£¨\s?10ì‹œê°„\s?ì´ˆê³¼|ì—°ì¥ê·¼ë¡œ.*ë¬´ì œí•œ)", t),
    "ë¬´ê¸‰ íœ´ì¼": lambda t: "ë¬´ê¸‰" in t and "íœ´ì¼" in t,
    "ì„ê¸ˆ ë¯¸ì§€ê¸‰ ê°€ëŠ¥ì„±": lambda t: re.search(r"(ì„ê¸ˆ.*ì§€ê¸‰í•˜ì§€.*ì•ŠëŠ”ë‹¤|ë¬´ê¸‰ê·¼ë¡œ|ì§€ì—°ì§€ê¸‰)", t)
}

def analyze(text):
    results = {}
    words = text.split()
    for category, keywords in required_fields.items():
        found = []
        for keyword in keywords:
            if any(is_similar(word, keyword) for word in words):
                found.append(keyword)
        if found:
            results[category] = found
    return results

def detect_missing(found):
    missing = {}
    for cat, keywords in required_fields.items():
        included = found.get(cat, [])
        remain = list(set(keywords) - set(included))
        if remain:
            missing[cat] = remain
    return missing

def detect_violations(text):
    return [label for label, rule in law_violation_rules.items() if rule(text)]

# â˜… Streamlit App
st.set_page_config(page_title="ê·¼ë¡œê³„ì•½ì„œ ìë™ ë¶„ì„ê¸°", layout="wide")
st.title("ğŸ“„ ê·¼ë¡œê³„ì•½ì„œ ìë™ ë¶„ì„ê¸° (OCR + ì „ì²˜ë¦¬ + ìœ„ë°˜ íŒë‹¨)")

uploaded_file = st.file_uploader("ğŸ“¤ ê³„ì•½ì„œ ì´ë¯¸ì§€ ì—…ë¡œë“œ (.png, .jpg)", type=["png", "jpg", "jpeg"])

if uploaded_file:
    st.image(uploaded_file, caption="ì—…ë¡œë“œëœ ì´ë¯¸ì§€", use_column_width=True)
    with st.spinner("ğŸ§  OCR ë¶„ì„ ì¤‘..."):
        files = {"image": (uploaded_file.name, uploaded_file.getvalue())}
        response = requests.post("https://ap-northeast-2.apistage.ai/v1/document-ai/ocr", headers=HEADERS, files=files)

        if response.status_code == 200:
            ocr_result = response.json()
            text = " ".join([p.get("text", "") for p in ocr_result.get("pages", [])])
            clean_text = preprocess(text)

            st.subheader("ğŸ§¾ ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸")
            st.text(clean_text[:1000] + "..." if len(clean_text) > 1000 else clean_text)

            found = analyze(clean_text)
            missing = detect_missing(found)
            violations = detect_violations(clean_text)

            st.subheader("âœ… í¬í•¨ëœ í•„ìˆ˜ í•­ëª©")
            st.json(found)

            st.subheader("âš ï¸ ëˆ„ë½ëœ í•­ëª©")
            if missing:
                st.json(missing)
            else:
                st.success("ëª¨ë“  í•­ëª© í¬í•¨")

            st.subheader("âŒ ì •ê·œì‹ ê¸°ë°˜ ìœ„ë°˜ ê°ì§€")
            if violations:
                for v in violations:
                    st.error(f"ğŸš¨ {v}")
            else:
                st.success("ëª…ì‹œì  ìœ„ë°˜ ì—†ìŒ")

            with st.expander("ğŸ” Upstage Embedding ê¸°ë°˜ ë²•ë¥  ì¡°í•­ ê²€ìƒ‰(RAG)"):
                rag_result = get_rag_result(clean_text)
                st.markdown(rag_result["result"])
                for i, doc in enumerate(rag_result["source_documents"]):
                    st.code(doc.page_content.strip(), language="markdown")
                    st.caption(f"ì¶œì²˜: {doc.metadata.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ')}")

            with st.expander("ğŸ§  solar-pro ëª¨ë¸ ê¸°ë°˜ ìœ„ë°˜ ì˜ë¯¸ ë¶„ì„"):
                llm_result = get_llm_judgment(clean_text)
                st.markdown(llm_result)

            st.download_button("ğŸ“¥ ë¶„ì„ ê²°ê³¼ PDF ì €ì¥", data=export_pdf(clean_text, found, missing, violations, rag_result, llm_result), file_name="ë¶„ì„ê²°ê³¼.pdf")

        else:
            st.error(f"âŒ OCR ì‹¤íŒ¨: {response.status_code}")
            st.text(response.text)
