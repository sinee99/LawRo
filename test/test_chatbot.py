import os
import time
import warnings
from dotenv import load_dotenv

# LangChain deprecation warnings ë¬´ì‹œ
warnings.filterwarnings("ignore", category=DeprecationWarning)

import streamlit as st
from langchain_upstage import UpstageEmbeddings, ChatUpstage
from langchain_chroma import Chroma
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

load_dotenv()

# â–¶ï¸ API í‚¤ í™•ì¸
api_key = os.getenv("UPSTAGE_API_KEY")
if not api_key:
    st.error("â—ï¸Upstage API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    st.stop()

# â–¶ï¸ ë²¡í„°ìŠ¤í† ì–´ ë¡œë”©
embedding = UpstageEmbeddings(
    model="solar-embedding-1-large",
    upstage_api_key=api_key
)

vectorstore = Chroma(
    persist_directory="storage/chroma_db",
    embedding_function=embedding
)
retriever = vectorstore.as_retriever(k=2)

# â–¶ï¸ ì±—ë´‡ ì´ˆê¸°í™”
chat = ChatUpstage(
    model="solar-pro",
    upstage_api_key=api_key
)

# â–¶ï¸ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
contextualize_q_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "ì´ì „ ëŒ€í™” ë‚´ìš©ê³¼ ì‚¬ìš©ì ì§ˆë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ, ë¬¸ë§¥ ì—†ì´ë„ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì§ˆë¬¸ì„ ë‹¤ì‹œ í‘œí˜„í•˜ì„¸ìš”."),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ]
)
history_aware_retriever = create_history_aware_retriever(chat, retriever, contextualize_q_prompt)

qa_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", 
         """ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ê·¼ë¡œê¸°ì¤€ë²• ë° ì™¸êµ­ì¸ê·¼ë¡œìê³ ìš© ë“±ì— ê´€í•œ ë²•ë¥  í•´ì„(Labor Law Guidance, ì´í•˜ LAG)ì„ ê¸°ë°˜ìœ¼ë¡œ ê·¼ë¡œê³„ì•½ì„œì˜ ì ë²•ì„± ì—¬ë¶€ë¥¼ íŒë‹¨í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì§ˆë¬¸ì— ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”. ë‹µì„ ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  í•˜ì„¸ìš”. ë‹µë³€ì€ ìƒì„¸í•˜ê³  ëª…í™•í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”. ë²•ë¥ ê³¼ ê´€ë ¨ëœ ì§ˆë¬¸ì—ë§Œ ë‹µë³€í•˜ì„¸ìš”. ê´€ë ¨ë˜ì§€ ì•Šì„ ê²½ìš° ë²•ë¥ ê³¼ ê´€ë ¨ëœ ì§ˆë¬¸ì„ í•  ìˆ˜ ìˆë„ë¡ ìœ ë„í•˜ëŠ” ë§ì„ í•´ì£¼ì„¸ìš”.

íŠ¹íˆ ë‹¤ìŒ ê¸°ì¤€ì„ ê³ ë ¤í•´ ì£¼ì„¸ìš”:

1. ìµœì €ì„ê¸ˆì€ ì‹œê°„ë‹¹ ê¸°ì¤€ìœ¼ë¡œ íŒë‹¨í•´ì£¼ì„¸ìš”.
2. ì£¼ 40ì‹œê°„ ê·¼ë¬´ ê¸°ì¤€ìœ¼ë¡œ ì›” í‰ê·  ê·¼ë¡œì‹œê°„ì€ 209ì‹œê°„ì…ë‹ˆë‹¤.
3. ì„ê¸ˆì´ ì‹œê¸‰/ì¼ê¸‰/ì›”ê¸‰ ì¤‘ ì–´ë–¤ í˜•ì‹ì´ë“  ì‹œê¸‰ìœ¼ë¡œ í™˜ì‚°í•˜ì—¬ ìµœì €ì„ê¸ˆ ì¶©ì¡± ì—¬ë¶€ë¥¼ íŒë‹¨í•´ ì£¼ì„¸ìš”.
4. ì‹ëŒ€Â·ìœ ë¥˜ë¹„Â·ìˆ™ì†Œ ë“± ë³µë¦¬í›„ìƒë¹„ëŠ” ìµœì €ì„ê¸ˆ ì‚°ì •ì— í¬í•¨ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
5. ìˆ˜ìŠµê¸°ê°„ ì¤‘ì—ë„ ìµœì €ì„ê¸ˆì€ ì ìš©ë˜ë©°, ê°ì•¡ ì‹œ ê·¼ë¡œê¸°ì¤€ë²• ì œ7ì¡°(ì°¨ë³„ ê¸ˆì§€) ìœ„ë°˜ ì†Œì§€ê°€ ìˆìŠµë‹ˆë‹¤.
6. ë‹¤ìŒ í•­ëª©ì´ ëˆ„ë½ë˜ì—ˆê±°ë‚˜ ë¶ˆëª…í™•í•˜ë‹¤ë©´ ì§€ì í•´ ì£¼ì„¸ìš”: ê·¼ë¡œê³„ì•½ê¸°ê°„, ê·¼ë¡œì‹œê°„, íœ´ê²Œì‹œê°„, íœ´ì¼, ì„ê¸ˆ, ì§€ê¸‰ë°©ë²• ë° ì‹œê¸°
7. ì™¸êµ­ì¸ ê·¼ë¡œìì¼ ê²½ìš°, ì²´ë¥˜ìê²©Â·ìˆ™ì†ŒÂ·í†µì—­ ì§€ì› ë“± ê³ ìš©í—ˆê°€ì œ ê´€ë ¨ ê¸°ì¤€ë„ ì°¸ê³ í•´ ì£¼ì„¸ìš”.

ğŸ“ ì¶œì²˜ ë¬¸ì„œ: {context}"""),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ]
)

question_answer_chain = create_stuff_documents_chain(chat, qa_prompt)
rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)

# â–¶ï¸ Streamlit UI
st.title("ğŸ“š LawRo ë²•ë¥  ìƒë‹´ Chatbot")

# â–¶ï¸ ëŒ€í™” ìƒíƒœ ê´€ë¦¬
if "messages" not in st.session_state:
    st.session_state.messages = []

# â–¶ï¸ ì´ì „ ë©”ì‹œì§€ ì¶œë ¥
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# â–¶ï¸ ì…ë ¥ ì²˜ë¦¬
MAX_MESSAGES = 6

if prompt := st.chat_input("ë²•ë¥  ê´€ë ¨ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”."):
    st.chat_message("user").markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    # â–¶ï¸ ìµœê·¼ Nê°œë§Œ ìœ ì§€
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""

        result = rag_chain.invoke({
            "input": prompt,
            "chat_history": st.session_state.messages
        })

        # â–¶ï¸ ì¦ê±°ìë£Œ ë‚´ìš© ì¶œë ¥
        docs = result.get("context", [])
        with st.expander("ğŸ“‚ ê´€ë ¨ ë¬¸ì„œ"):
            if isinstance(docs, list):
                for i, doc in enumerate(docs, 1):
                    content = getattr(doc, "page_content", "ì¶œì²˜ ì—†ìŒ")
                    st.markdown(f"**[{i}]** {content.strip()[:500]}...")  # ìµœëŒ€ 500ì

        # â–¶ï¸ ë‹µë³€ ì• ë‹ˆë©”ì´ì…˜ ì¶œë ¥
        for chunk in result["answer"].split(" "):
            full_response += chunk + " "
            message_placeholder.markdown(full_response + "â–Œ")
            time.sleep(0.02)
        message_placeholder.markdown(full_response)

    st.session_state.messages.append({"role": "assistant", "content": full_response})
