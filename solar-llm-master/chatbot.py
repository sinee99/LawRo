import os
from dotenv import load_dotenv
load_dotenv()

import streamlit as st
import time
from langchain_upstage import UpstageEmbeddings, ChatUpstage
from langchain_community.vectorstores import Chroma
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, SystemMessage

# â–¶ï¸ ë²¡í„° ì €ì¥ëœ ë””ë ‰í† ë¦¬ì—ì„œ ë¶ˆëŸ¬ì˜¤ê¸°
embedding = UpstageEmbeddings(
    model="solar-embedding-1-large",
    upstage_api_key=os.getenv("UPSTAGE_API_KEY")
)

vectorstore = Chroma(
    persist_directory="chroma_db",
    embedding_function=embedding
)
retriever = vectorstore.as_retriever(k=2)

# â–¶ï¸ ì±—ë´‡ ì´ˆê¸°í™”
chat = ChatUpstage(
    model="solar-pro",
    upstage_api_key=os.getenv("UPSTAGE_API_KEY")
)

# â–¶ï¸ ë¬¸ë§¥ ì¬êµ¬ì„± í”„ë¡¬í”„íŠ¸
contextualize_q_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", 
         "ì´ì „ ëŒ€í™” ë‚´ìš©ê³¼ ì‚¬ìš©ì ì§ˆë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ, ë¬¸ë§¥ ì—†ì´ë„ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì§ˆë¬¸ì„ ë‹¤ì‹œ í‘œí˜„í•˜ì„¸ìš”."),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ]
)

history_aware_retriever = create_history_aware_retriever(
    chat, retriever, contextualize_q_prompt
)

# â–¶ï¸ ë¬¸ì„œ ê¸°ë°˜ QA í”„ë¡¬í”„íŠ¸
qa_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", 
         """ì§ˆë¬¸ì— ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”. ë‹µì„ ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  í•˜ì„¸ìš”. ë‹µë³€ì€ ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”. ë²•ë¥ ê³¼ ê´€ë ¨ëœ ì§ˆë¬¸ì—ë§Œ ë‹µë³€í•˜ì„¸ìš”. ê´€ë ¨ë˜ì§€ ì•Šì„ê²½ìš° ë²•ë¥ ê³¼ ê´€ë ¨ëœ ì§ˆë¬¸ì„ í•  ìˆ˜ ìˆë„ë¡ ìœ ë„í•˜ëŠ” ë§ì„ í•´ì£¼ì„¸ìš”.
         
ğŸ“ë‹µë³€ ë‚´ìš©:  
ğŸ“ì¶œì²˜ ë¬¸ì„œ: {context}"""),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ]
)

question_answer_chain = create_stuff_documents_chain(chat, qa_prompt)
rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)

# â–¶ï¸ Streamlit ì¸í„°í˜ì´ìŠ¤
st.title("ğŸ“š LawRo ë²•ë¥  ìƒë‹´ Chatbot")

if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

MAX_MESSAGES = 4

if prompt := st.chat_input("ë²•ë¥  ê´€ë ¨ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”."):
    if len(st.session_state.messages) >= MAX_MESSAGES:
        del st.session_state.messages[0]
        del st.session_state.messages[0]

    st.chat_message("user").markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""

        result = rag_chain.invoke({
            "input": prompt,
            "chat_history": st.session_state.messages
        })

        # â–¶ï¸ ë¬¸ì„œ ì¶œì²˜ ì¶”ì¶œ
        sources = set()
        for doc in result.get("context", []):
            src = doc.metadata.get("source", "Unknown")
            sources.add(src)

        answer = result["answer"]
        sources_list = "\n".join(f"ğŸ“„ {src}" for src in sources)
        final_response = f"{answer.strip()}\n\nğŸ” **ì¶œì²˜ ë¬¸ì„œ:**\n{sources_list}"

        for chunk in final_response.split(" "):
            full_response += chunk + " "
            message_placeholder.markdown(full_response + "â–Œ")
            time.sleep(0.02)
        message_placeholder.markdown(full_response)

    st.session_state.messages.append({"role": "assistant", "content": full_response})
